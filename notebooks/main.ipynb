{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "recipesdf = pd.read_csv(\"recipes.csv\")\n",
    "reviewsdf = pd.read_csv(\"reviews.csv\")\n",
    "reviewsdf = reviewsdf.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 3356\n",
      "Number of unique recipes: 5514\n",
      "The full rating matrix will have: 18504984 elements.\n",
      "----------\n",
      "Number of ratings: 10000\n",
      "Therefore:  0.05403949552185509 % of the matrix is filled.\n",
      "We have an incredibly sparse matrix to work with here.\n",
      "And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\n",
      "You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\n",
      "One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\n"
     ]
    }
   ],
   "source": [
    "recept_names = recipesdf.set_index('RecipeId')['Name'].to_dict()\n",
    "n_users = len(reviewsdf.AuthorId.unique())\n",
    "n_items = len(reviewsdf.RecipeId.unique())\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique recipes:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "print('----------')\n",
    "print(\"Number of ratings:\", len(reviewsdf))\n",
    "print(\"Therefore: \", len(reviewsdf) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
    "print(\"We have an incredibly sparse matrix to work with here.\")\n",
    "print(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
    "print(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
    "print(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness\n",
    "\n",
    "# Note: This isn't 'good' practice, in a MLops sense but we'll roll with this since the data is already loaded in memory.\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = reviewsdf.copy()\n",
    "        \n",
    "        # Extract all user IDs and recipe IDs\n",
    "        users = reviewsdf.AuthorId.unique()\n",
    "        recipes = reviewsdf.RecipeId.unique()\n",
    "        \n",
    "        #--- Producing new continuous IDs for users and recipes ---\n",
    "        \n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.recipeid2idx = {o:i for i,o in enumerate(recipes)}\n",
    "        \n",
    "        # Obtained continuous ID for users and recipes\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2recipeid = {i:o for o,i in self.recipeid2idx.items()}\n",
    "        \n",
    "        # return the id from the indexed values as noted in the lambda function down below.\n",
    "        self.ratings.RecipeId = reviewsdf.RecipeId.apply(lambda x: self.recipeid2idx[x])\n",
    "        self.ratings.AuthorId = reviewsdf.AuthorId.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['ReviewId','AuthorName','Rating','Review','DateSubmitted','DateModified'], axis=1).values\n",
    "        self.y = self.ratings['Rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(3356, 8)\n",
      "  (item_factors): Embedding(5514, 8)\n",
      ")\n",
      "user_factors.weight tensor([[0.0122, 0.0230, 0.0450,  ..., 0.0181, 0.0164, 0.0018],\n",
      "        [0.0048, 0.0212, 0.0240,  ..., 0.0223, 0.0300, 0.0148],\n",
      "        [0.0105, 0.0299, 0.0164,  ..., 0.0186, 0.0245, 0.0118],\n",
      "        ...,\n",
      "        [0.0145, 0.0172, 0.0006,  ..., 0.0293, 0.0052, 0.0471],\n",
      "        [0.0274, 0.0072, 0.0164,  ..., 0.0276, 0.0046, 0.0170],\n",
      "        [0.0405, 0.0024, 0.0397,  ..., 0.0323, 0.0071, 0.0018]])\n",
      "item_factors.weight tensor([[0.0045, 0.0021, 0.0212,  ..., 0.0477, 0.0295, 0.0098],\n",
      "        [0.0230, 0.0036, 0.0030,  ..., 0.0093, 0.0137, 0.0215],\n",
      "        [0.0182, 0.0427, 0.0451,  ..., 0.0040, 0.0495, 0.0362],\n",
      "        ...,\n",
      "        [0.0449, 0.0314, 0.0461,  ..., 0.0358, 0.0101, 0.0259],\n",
      "        [0.0241, 0.0185, 0.0013,  ..., 0.0193, 0.0016, 0.0032],\n",
      "        [0.0203, 0.0014, 0.0200,  ..., 0.0225, 0.0183, 0.0302]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfaaa01e0084b269a624f9be24e6ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for it in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "         if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "    #print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[0.0122, 0.0230, 0.0450,  ..., 0.0181, 0.0164, 0.0018],\n",
      "        [0.0048, 0.0212, 0.0240,  ..., 0.0223, 0.0300, 0.0148],\n",
      "        [0.0105, 0.0299, 0.0164,  ..., 0.0186, 0.0245, 0.0118],\n",
      "        ...,\n",
      "        [0.0145, 0.0172, 0.0006,  ..., 0.0293, 0.0052, 0.0471],\n",
      "        [0.0274, 0.0072, 0.0164,  ..., 0.0276, 0.0046, 0.0170],\n",
      "        [0.0405, 0.0024, 0.0397,  ..., 0.0323, 0.0071, 0.0018]])\n",
      "item_factors.weight tensor([[0.0045, 0.0021, 0.0212,  ..., 0.0477, 0.0295, 0.0098],\n",
      "        [0.0230, 0.0036, 0.0030,  ..., 0.0093, 0.0137, 0.0215],\n",
      "        [0.0182, 0.0427, 0.0451,  ..., 0.0040, 0.0495, 0.0362],\n",
      "        ...,\n",
      "        [0.0449, 0.0314, 0.0461,  ..., 0.0358, 0.0101, 0.0259],\n",
      "        [0.0241, 0.0185, 0.0013,  ..., 0.0193, 0.0016, 0.0032],\n",
      "        [0.0203, 0.0014, 0.0200,  ..., 0.0225, 0.0183, 0.0302]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5514"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By training the model, we will have tuned latent factors for recipes and users.\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "          uw = param.data\n",
    "          c +=1\n",
    "        else:\n",
    "          iw = param.data\n",
    "        #print('param_data', param_data)\n",
    "\n",
    "trained_recipe_embeddings = model.item_factors.weight.data.cpu().numpy()\n",
    "\n",
    "len(trained_recipe_embeddings) # unique recipe factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Belly Buster\n",
      "\t Pecan Pie Muffins\n",
      "\t Melt in Your Mouth Banana Bread\n",
      "\t Crock Pot Garlic Brown Sugar Chicken\n",
      "\t Baked Pasta with Asparagus (Pasta al Forno con Asparagi)\n",
      "\t Easy Pot Roast\n",
      "\t Quick and Easy No Bake Chocolate Cookies\n",
      "\t Pepsi Pork Roast\n",
      "\t BBQ Ribs\n",
      "\t Crock Pot Maple Country Ribs\n",
      "Cluster #1\n",
      "\t Steven's World Famous To-Die-For Sour Cream Chicken\n",
      "\t Slow-Cooker Cheesy Chicken\n",
      "\t Dutch Cucumber Salad\n",
      "\t Chicken Tortilla Soup II\n",
      "\t Creamy Taco Casserole\n",
      "\t Bleu Cheese Beef Tenderloin\n",
      "\t Mary's Best Zucchini Bread\n",
      "\t Creamy Beef Stroganoff Over Rice\n",
      "\t No Fuss No Mess Potato Pancakes\n",
      "\t Hot German Potato Salad\n",
      "Cluster #2\n",
      "\t Yummy Banana Bread\n",
      "\t The Bomb Burgers\n",
      "\t Garlic Chicken Breasts in Balsamic Vinegar\n",
      "\t Betty White's Chicken Wings\n",
      "\t Asparagus with Toasted Pine Nuts & Lemon Vinaigrette\n",
      "\t Easy Chicken & Potato Dinner\n",
      "\t Depression Fudge Cake\n",
      "\t Fantasy Fudge\n",
      "\t Creamy Chicken and Penne\n",
      "\t Spinach and Strawberry Salad\n",
      "Cluster #3\n",
      "\t Cracker Barrel's Hashbrowns Casserole - Copycat\n",
      "\t Chicken Stroganoff - Crock Pot\n",
      "\t The Best Pork Chop Dinner - EVER!\n",
      "\t Chicken, Broccoli and Rice Casserole\n",
      "\t Best Ever Meatloaf\n",
      "\t Roast \"Sticky\" Chicken\n",
      "\t Greek Death by Garlic Pasta\n",
      "\t Tortilla Dip Supreme\n",
      "\t Buttery Farm Biscuits\n",
      "\t Chipotle Pork Roast\n",
      "Cluster #4\n",
      "\t Should Be Illegal Oven BBQ Ribs\n",
      "\t Best Banana Bread\n",
      "\t Chicken Parmesan\n",
      "\t Ham Chowder\n",
      "\t Best Hamburger Diane\n",
      "\t Quick Quesadillas\n",
      "\t Banana Cake\n",
      "\t Good-Morning Pancakes\n",
      "\t Hot Artichoke Dip\n",
      "\t Cherry Fluff Salad\n",
      "Cluster #5\n",
      "\t Human Puppy Chow\n",
      "\t Easy French Onion Soup\n",
      "\t Beef & Cheese Husband Delight\n",
      "\t Wholly Guacamole\n",
      "\t Raspberry Sherbet Punch\n",
      "\t Yummy and Comforting Chicken Tetrazzini\n",
      "\t Cranberry Bars\n",
      "\t Awful Potatoes\n",
      "\t Velveeta Cheese Fudge\n",
      "\t Baklava\n",
      "Cluster #6\n",
      "\t Jo Mama's World Famous Spaghetti\n",
      "\t Pumpkin Cream Cheese Roll\n",
      "\t Spinach Meatloaf\n",
      "\t Crock Pot Pork Roast and Mushrooms\n",
      "\t Neiman-Marcus $250 Chocolate Chip Cookies Recipe\n",
      "\t Upside-Down Fresh Peach Cobbler\n",
      "\t Chocolate Eclair Dessert\n",
      "\t My Mom's Wilted Lettuce\n",
      "\t Easy Deviled Eggs\n",
      "\t Pork Roast over Sweet Potatoes for the Crock Pot\n",
      "Cluster #7\n",
      "\t Pepsi Chicken\n",
      "\t Beef Roast\n",
      "\t Ranch Potatoes\n",
      "\t Easy Banana Muffins\n",
      "\t Red Pepper Dip\n",
      "\t Greek-Style Oven Fries\n",
      "\t Balsamic Chicken\n",
      "\t 3rd Serving Meatloaf\n",
      "\t Easy Omelet for One\n",
      "\t Italian Casserole Dinner\n",
      "Cluster #8\n",
      "\t Crisp Bacon Potatoes\n",
      "\t BBQ Pot Ribs\n",
      "\t Okonomiyaki - Japanese Pizza\n",
      "\t Nutty Green Beans\n",
      "\t Shepherd's Pie\n",
      "\t Corn Dog Casserole\n",
      "\t Cheesy Cincinnati Chili\n",
      "\t Carol's Creation\n",
      "\t Mexicali Spoon Bread Casserole\n",
      "\t Honey Mustard Curry Chicken\n",
      "Cluster #9\n",
      "\t Wonderful Salsa\n",
      "\t Fried Rice\n",
      "\t Potato Soup\n",
      "\t Twinkie Del Mar\n",
      "\t Kevin's Best Corned Beef\n",
      "\t Chili Con Carne With Beans\n",
      "\t Classic Baked Ziti\n",
      "\t Potato Mushroom Casserole\n",
      "\t Traditional Meatloaf\n",
      "\t Mozzarella, Tomato and Basil Salad\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the recipe weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_recipe_embeddings)\n",
    "\n",
    "'''It can be seen here that the recipes that are in the same cluster tend to have\n",
    "similar genres. Also note that the algorithm is unfamiliar with the recipe name\n",
    "and only obtained the relationships by looking at the numbers representing how\n",
    "users have responded to the recipe selections.'''\n",
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  movs = []\n",
    "  for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    movid = train_set.idx2recipeid[movidx]\n",
    "    rat_count = reviewsdf.loc[reviewsdf['RecipeId']==movid].count()[0]\n",
    "    movs.append((recept_names[movid], rat_count))\n",
    "  for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", mov[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a63501da4ff5345f792f8d85fdbe5e308a00ec1c83390f9577f8c8a1adb4af3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
